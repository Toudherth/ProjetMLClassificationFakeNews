# -*- coding: utf-8 -*-
"""DéveloppementProjet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TVTSmtpc6ujdgHdznXXMbgTZVPsYlldC

# HAI817I - Machine Learning 1 (méthodes classiques): Classification de fake news


**Toudherth MEKHNACHE (22214610)**

**Lorenzo PUCCIO (21904376)**

**Arthur SAPIN (22200824)**

**Vithya SOCCALINGAME (22210930)**

##Imports, définition de variables et dataframe

Après avoir géré les imports et le montage sur Google Drive, nous définissons plusieurs variables globales utilisées tout au long du projet, ainsi que trois dataframes différents.

Le premier dataframe est tout simplement issu de la lecture des fichiers CSV d'entraînement et de test, fournis sur la page Moodle de l'unité d'enseignement.

Le second dataframe est un dataframe temporaire sur lequel on effectue toutes nos modifications. Il nous permet d'explorer plusieurs pistes expérimentales au niveau du prétraitement.

Le troisième dataframe est notre dataframe final, celui que l'on va utiliser pour la classification. Il est instancié comme étant vide durant cette étape.
"""

# Installations nécessaires au bon fonctionnement du notebook

!pip install pyLDAvis

# Imports

# Utilisation générale
import pandas as pd
import matplotlib.pyplot as plt
import sys
import numpy as np
import sklearn

# Imports concernant le pré-traitement
import re
import unicodedata
from bs4 import BeautifulSoup
import nltk
nltk.download('punkt')
from nltk import sent_tokenize
from nltk.tokenize import word_tokenize
import inflect
from nltk import pos_tag
nltk.download('averaged_perceptron_tagger')
nltk.download('tagsets')
nltk.download("stopwords")
from nltk.corpus import stopwords
nltk.download('omw-1.4')
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer

# Imports concernant la vectorisation TF-IDF
import warnings
from random import uniform
import itertools
import seaborn as sn
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import time
import string
import base64
import pyLDAvis
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
import spacy
import gensim
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel
from gensim.models import Phrases
from gensim.models.phrases import Phraser
from gensim import corpora
from gensim import models

# Imports concernant la classification
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score

# Définition de variables

# Affiche les classes grammaticales de NLTK
nltk.help.upenn_tagset()

# Toutes les classes grammaticales de NLTK dans une liste
alltags = ["$", "''", "(", ")", ",", "--", ".", ":",
           "CC", "CD", "DT", "EX", "FW", "IN", "JJ", "JJR", "JJS",
           "LS", "MD", "NN", "NNP", "NNPS", "NNS", "PDT", "POS", "PRP", "PRP$",
           "RB", "RBR", "RBS", "RP", "SYM", "TO", "UH",
           "VB", "VBD", "VBG", "VBN", "VBP", "VBZ", "WDT", "WP", "WP$", "WRB", "``"]

# Liste des classes grammaticales correspondant à la ponctuation
punctuators = ['\'\'', '(', ')', ',', '--', '.', ':']

# Liste des stopwords en anglais
the_stopwords=set(stopwords.words("english"))
print(the_stopwords)

# Racinisateur de NLTK
ps=nltk.stem.porter.PorterStemmer()

# Lemmatisateur de NLTK
lem = nltk.stem.wordnet.WordNetLemmatizer()

# Montage de Google Drive
from google.colab import drive
drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
# Ajoutez un raccourci du dossier Projet dans votre dossier Google Colab de votre Drive, puis lancez cette cellule
my_local_drive='/content/gdrive/My Drive/Colab Notebooks/Projet'

# Ajout du chemin d'accès pour les librairies, fonctions et données
sys.path.append(my_local_drive)

# Se positionne sur le répertoire associé
# %cd $my_local_drive
# %pwd

# Affichage, calculs, évaluations
from MyNLPUtilities import *

# Dataframe de base (issu des fichiers CSV fournis sur Moodle)
df=pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/Projet/Project_data/HAI817_Projet_train.csv', sep=',')
df2=pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/Projet/Project_data/HAI817_Projet_test.csv', sep=',')
df3=pd.concat([df, df2], ignore_index=True)
df=df3.reset_index(drop=True)
df=df.drop('ID', axis=1)
df=df.drop('public_id', axis=1)
display(df.head())
print(df.info())

# Dataframe temporaire
tempdf=df.copy(deep=True)
display(tempdf.head())

# Dataframe final (vide pour l'instant)
finaldf = pd.DataFrame()

# Affichages des différentes classes de vérité, selon lequelles on classera les données dans l'étape 3

print(df['our rating'].value_counts())

"""## Etape 1 - Ingénierie des données: Pré-traitement

L'étape de pré-traitement est essentielle afin de faciliter le *feature engineering* et la classification. L'objectif est notamment de filtrer les données textuelles afin de n'en garder que l'essentiel pour la classification.

Nous avons séparé les différentes étapes du pré-traitement des données en plusieurs sections différentes afin d'indiquer les modifications que nous effectuons sur notre dataframe temporaire.

**Encodage des données**

L'encodage est essentiel à la standardisation des données textuelles: elle nettoie le texte des caractères qui auraient été mal rendus dans la transition entre la publication de l'article et la collecte des données.
"""

# Encodage des cellules de la colonne 'text' en ASCII
tempdf['text'] = tempdf['text'].apply(lambda x: unicodedata.normalize('NFKD', x).encode('ascii', 'ignore'))
display(tempdf)

"""**Suppression des tags HTML**

Notre dataset est composé d'articles issus de différents sites web. On considère qu'éventuellement, et quelle que soit la raison, le texte d'un article contienne des balises HTML qui appartiennent normalement à la structure du site web. Il est évident que ces balises ne sont pas censées appartenir au texte de l'article; il convient donc de les retirer.
"""

# Suppression des balises et tags HTML avec BeautifulSoup
def strip_html(text):
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text()

# Application de strip_html sur la colonne 'text'
tempdf['text'] = tempdf['text'].apply(lambda x: strip_html(x))
display(tempdf)

"""**Utilisation d'expressions régulières**

Nous avons décidé de supprimer les liens hypertexte et les adresses email du texte des articles. On estime que bien que le contenu des hyperliens pourrait être intéressant, il n'est pas garanti que l'URL du lien indique l'information que la page web associée comporte. Les adresses email indiquent en général le nom d'une personne, mais la personne concernée est en général l'auteur de l'article ou une personne associée; les adresses email sont alors perturbatrices à la capture de l'information que l'on cherche.
"""

# Suppression des hyperliens
def remove_hyperlink(text):
  return re.sub('(www\.|http)\S+', '', text)

# Suppression des adresses email
def remove_email(text):
  return re.sub('\S+@\S+', '', text)

# Création de la colonne treatedtext: on appliquera d'autres modifications dessus pendant cette étape

# Suppression des liens hypertexte
tempdf['treatedtext'] = tempdf['text'].apply(lambda x: remove_hyperlink(x))
display(tempdf)

# Suppression des adresses email
tempdf['treatedtext'] = tempdf['treatedtext'].apply(lambda x: remove_email(x))
display(tempdf)

"""**Découpage en tokens (tokenisation) avec NLTK**

La bibliothèque NLTK de Python permet de produire des jetons à partir d'un texte. Ici, on utilise la bibliothèque NLTK pour créer des jetons sous forme de phrases, puis chacune des phrases devient une série de jetons composés en général d'un mot et d'une classe grammaticale. Ensuite, on exploite la classification grammaticale pour sélectionner certains mots et se débarasser de mots superflus (ici appelés *stopwords*). Enfin, on utilise les objets PorterStemmer et WordNetLemmatizer pour lemmatiser puis raciniser chacun des mots des tokens.
"""

# Tokénisation du texte en phrases
def tokenize_sentences(text):
  return sent_tokenize(text)

# Tokénisation des phrases en tokens classifiés grammaticalement avec NLTK
def tokenize_words(array):
  totaltokens = []
  for text in array:
    tokens = []
    for sentence in text:
      senttokens = word_tokenize(sentence)
      senttokens_tag = nltk.pos_tag(senttokens)
      for token in senttokens_tag:
        tokens.append(token)
    totaltokens.append(tokens)
  return totaltokens

# Nettoyage de la ponctuation et les symboles qui ne sont pas classés en tant que tel
def clean_punctuation_symbols(tokenlist):
  newtokenlist = tokenlist
  for token in newtokenlist:
    if bool(re.search("[(\[{)\]},\-\-.!?:;*$£€¥%&'+/<=>@]|\.\.\.|\"", token[0])) and (token[1] not in punctuators or token[1] != 'SYM'):
      re.sub("[(\[{)\]},\-\-.!?:;*$£€¥%&'+/]|\.\.\.|\"", '', token[0])
  return newtokenlist

# Suppression de tous les stopwords de la liste des tokens
def remove_stopwords(tokenlist):
  newtokenlist = []
  for token in tokenlist:
    if token[0] not in the_stopwords:
      newtokenlist.append(token)
  return newtokenlist

# Sélectionne des tokens de classes grammaticales précises
# Par défaut, on veut garder nombres, mots hors anglais, adjectifs, noms communs/propres, adverbes, verbes
def select_tokens(tokenlist, tags=['CD', 'FW', 'JJ', 'JJR', 'JJS', 'NN', 'NNP', 'NNS', 'NNPS', 'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']):
  newtokenlist = []
  for token in tokenlist:
    if token[1] in tags:
      newtokenlist.append(token)
  return newtokenlist

# Déclassage grammatical, lemmatisation puis stemming
def untag_lemmatize_stem_tokens(tokenlist):
  forbiddentags = ['FW', 'NNP', 'NNPS']
  lemmatizedtokens = []
  stemmedtokens = []
  for token in tokenlist:
    if token[1] not in forbiddentags:
      lemmatizedtokens.append((lem.lemmatize(token[0]), token[1]))
    else:
      lemmatizedtokens.append(token)
  for token in lemmatizedtokens:
    if token[1] not in forbiddentags:
      stemmedtokens.append(ps.stem(token[0]))
    else:
      stemmedtokens.append(token[0])
  return stemmedtokens

# Tokénisation des phrases, passées en minuscule pour faciliter le filtrage de certains mots
tempdf['treatedsentences'] = tempdf['treatedtext'].apply(lambda x: x.lower()).apply(tokenize_sentences)
display(tempdf)

# Tokénisation des mots classés grammaticalement, stockés dans la colonne treatedtokens
tempdf['treatedtokens'] = tokenize_words(tempdf['treatedsentences'])
display(tempdf)

# Nettoyage de la ponctuation et des symboles superflus
tempdf['treatedtokens'] = tempdf['treatedtokens'].apply(lambda x: clean_punctuation_symbols(x))
display(tempdf)

# Suppression des stopwords
tempdf['treatedtokens'] = tempdf['treatedtokens'].apply(lambda x: remove_stopwords(x))
display(tempdf)

# Sélection des tokens que l'on souhaite garder

tempdf['treatedtokens'] = tempdf['treatedtokens'].apply(lambda x: select_tokens(x))
display(tempdf)

# Derniers traitements: lemmatisation, déclassage grammatical, stemming

tempdf['treatedtokens'] = tempdf['treatedtokens'].apply(lambda x: untag_lemmatize_stem_tokens(x))
display(tempdf)

"""**Dataframe final**

On garde le résultat de toutes les étapes de pré-traitement effectuées dans un dataframe. On conserve une liste de mots (donc une liste de string) plutôt que le corps entier de l'article comme le dataframe initial. Ce dataframe final sera notamment utilisé pour la vectorisation de l'étape 2.
"""

# Ajout de colonnes pour le dataframe final

finaldf['title'] = tempdf['title']
finaldf['finaltokens'] = tempdf['treatedtokens']
finaldf['rating'] = tempdf['our rating']

display(finaldf)

"""## Etape 2 - Ingénierie des données: Vectorisation
Cette étape d'ingénierie de données permet de quantifier les caractéristiques des données textuelles avant de les utiliser dans un modèle de classification. Elle inclut diverses techniques pour ne garder que les données les plus pertinentes. Cette étape est donc importante pour obtenir des résultats précis et cohérents.


"""

# Copie du dataframe final sur laquelle on va travailler pour l'étape 2
df_copy = finaldf.copy()

"""**Matrice TF-IDF**

Pour cela, une matrice TF-IDF est créée à partir d'un dataframe copié. La fonction TfidfVectorizer de scikit-learn est utilisée pour créer la matrice, qui est ensuite représentée sous forme de heatmap en utilisant la librairie seaborn. On utilisera ainsi le résultat obtenu pour entraîner notre modèle de classification de fake news.
"""

# Définir un objet Vectorizer en utilisant la méthode TF-IDF
vectorizer = TfidfVectorizer(max_features=20)

# Transformer les données textuelles en une matrice
X = vectorizer.fit_transform(df_copy['finaltokens'].apply(' '.join))

# Récupèrer les noms des caractéristiques créées par le Vectorizer
feature_names = vectorizer.get_feature_names_out()

# Créer un dataframe à partir de la matrice de caractéristiques normalisées
df_copy_tf_idf = pd.DataFrame(data=X.toarray(), columns=feature_names)
display(df_copy_tf_idf)

# Créer une visualisation en heatmap
sns.heatmap(df_copy_tf_idf, cmap='Blues', vmin=0, vmax=1).set_title('Matrice')

"""**Topic modelling**

Le Topic Modelling est une technique d'analyse de texte qui permet d'identifier des thèmes ou des sujets cachés dans un corpus de documents en regroupant les mots en fonction de leur co-occurrence et de leur distribution statistique.

Ainsi après avoir installé et importé les bibliothèques nécessaires pour la modélisation de sujets et l'analyse de texte, nous créeons des bigrammes et des trigrammes pour améliorer la qualité des résultats de modélisation, et utilisons des dictionnaires et des corpus pour entraîner le modèle LDA. Nous utilisons également la transformation TF-IDF pour mesurer l'importance relative d'un terme dans un document. Pour finir, la fonction MyCleanTextsforLDA est utilisée pour nettoyer les textes avant la modélisation, et la méthode LDA de gensim est utilisée pour modéliser les sujets.
"""

# Création des bigrammes et trigrammes
docs = df_copy['finaltokens'].tolist()
bigram = gensim.models.Phrases(docs, min_count=5, threshold=100)
trigram = gensim.models.Phrases(bigram[docs], min_count=5, threshold=100)
bigram_mod = gensim.models.phrases.Phraser(bigram)
trigram_mod = gensim.models.phrases.Phraser(trigram)

# Application des bigrammes et trigrammes aux documents
docs_bigrams = [bigram_mod[doc] for doc in docs]
docs_trigrams = [trigram_mod[bigram_mod[doc]] for doc in docs_bigrams]

# Récupération des différents composants des phrases
bigram_tokens = []
for doc in docs:
    bigram_tokens_doc = []
    for token in doc:
        bigram_tokens_token = bigram_mod[token]
        bigram_tokens_token = [t for t in bigram_tokens_token if '_' in t]  # ne récupère que les bigrammes
        bigram_tokens_doc.append((token, bigram_tokens_token))
    bigram_tokens.append(bigram_tokens_doc)

trigram_tokens = []
for doc in docs_trigrams:
    trigram_tokens_doc = []
    for token in doc:
        trigram_tokens_token = trigram_mod[token]
        trigram_tokens_token = [t for t in trigram_tokens_token if '_' in t]  # ne récupère que les trigrammes
        trigram_tokens_doc.append((token, trigram_tokens_token))
    trigram_tokens.append(trigram_tokens_doc)

# Affichage des résultats
print("Les phrases et les bi/trigrammes associés :\n")
for i in range(len(bigram_tokens)):
    print("Phrase :", docs[i])
    print("Bigrammes associés :", bigram_tokens[i])
    print("Trigrammes associés :", trigram_tokens[i])
    print("\n")

max_words = 1000

dictionnaire = gensim.corpora.Dictionary(docs_trigrams)
dictionnaire.filter_extremes(no_below=5, no_above=0.5, keep_n=max_words)

# pour connaitre le nombre de chaque token
print("Les différents mots du dictionnaire : \n",dictionnaire.token2id)

# conversion des mots extraits du texte en vecteur de type bag of words
corpus = [dictionnaire.doc2bow(text) for text in docs_trigrams]

print ("\nLa matrice obtenue après tranformation où chaque ligne correspond à un document \n")
for doc in corpus:
    print([[dictionnaire[id], freq] for id, freq in doc])

# conversion via TF-IDF
tfidf_model = gensim.models.TfidfModel(corpus)
corpus_tfidf = tfidf_model[corpus]
print ("\nLa matrice obtenue après tranformation TF-IDF")
for doc in tfidf_model[corpus]:
    print([[dictionnaire[id], np.around(freq,2)] for id, freq in doc])

# si besoin de changer la valeur de max_words, il vaut mieux la changer ici
max_words = 1000

# définition d'une fonction pour vectoriser un texte avec TF-IDF
def MyCleanTextsforLDA(texts,
                      min_count=1, # nombre d'apparitions minimales pour un bigram
                      threshold=2,
                      no_below=1, # nombre minimum d'apparitions pour être dans le dictionnaire
                      no_above=0.5, # pourcentage maximal (sur la taille totale du corpus) pour filtrer
                      ):

    # utilisation de spacy pour ne retenir que les allowed_postags
    allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']
    nlp = spacy.load("en_core_web_sm", disable=['parser', 'ner'])
    sentences=texts.copy()
    texts_out = []
    for sent in sentences:
        if len(sent) < (nlp.max_length): # si le texte est trop grand
            doc = nlp(" ".join(sent))
            texts_out.append(" ".join([token.lemma_ for token in doc if token.pos_ in allowed_postags]))
        else:
            texts_out.append(sent)
    sentences=texts_out

    # suppression des stopwords
    stop_words = the_stopwords
    words =[[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in sentences]


    # recherche des bigrammes
    bigram = Phrases(words, min_count, threshold,delimiter=' ')
    bigram_phraser = Phraser(bigram)


    # sauvergarde des tokens et des bigrammes
    bigram_token = []
    for sent in words:
        bigram_token.append(bigram_phraser[sent])


    # création du vocabulaire avec une limite de 5000 mots
    dictionary = corpora.Dictionary(bigram_token)
    dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=max_words)

    # conversion des tokens en vecteur de type bag of words
    corpus = [dictionary.doc2bow(text) for text in bigram_token]

    # transformation via TF-IDF
    tfidf_model = models.TfidfModel(corpus)
    corpus_tfidf = tfidf_model[corpus]

    return corpus, corpus_tfidf, dictionary, bigram_token

num_topics=10 # nombre de topics
num_words=10 # nombre de mots par topics

# Préparation du texte pour LDA
corpus, corpus_tfidf, dictionary, bigram_token = MyCleanTextsforLDA(docs)

lda_model_bow = gensim.models.ldamulticore.LdaMulticore(
                           corpus=corpus_tfidf,
                           num_topics=num_topics,
                           id2word=dictionary,
                           chunksize=100,
                           workers=7,
                           passes=10,
                           random_state=100,
                           eval_every = 1,
                           per_word_topics=True)


print ("Affichage des ",num_topics," différents topics pour le corpus  :\n")
for idx, topic in lda_model_bow.print_topics(-1,num_words):
    print('Topic: {} Word: {}'.format(idx, topic))

"""## Etape 3 - Tâches de classification

Après toutes les étapes de pré-traitement et de vectorisation, les données textuelles à traiter sont enfin prêtes à être classifiées. Cependant, en choisissant l'approche des pipelines de traitement pour l'ingénierie des données, on ne peut pas encore classifier les données. Un travail en amont doit être fait pour personnaliser les différentes étapes de pré-traitement afin d'obtenir des données diverses et pertinentes.

**Définition de fonctions supplémentaires**

Dans l'étape de pré-traitement, nous avons défini plusieurs fonctions qui s'appuient sur la tokénisation grammaticale avec NLTK. Or, puisque cette tokénisation grammaticale fait partie de nos différentes étapes de prétraitement, il serait préférable de choisir si l'on souhaite ou non l'utiliser dans une pipeline. Ainsi, on définira une alternative à la tokénisation grammaticale: une tokénisation classique qui séparera les phrases en mots. On en profitera également pour séparer la fonction de pré-traitement `untag_lemmatize_stem_tokens` et définir des fonctions équivalentes aux fonctions de pré-traitement utilisées après la tokénisation avec NLTK, pour notre tokénisateur classique.
"""

# Fonctions visant à séparer untag_lemmatize_stem en trois fonctions différentes

# Lemmatisation en NLTK
def lemmatize_tokens(tokenlist):
  forbiddentags = ['FW', 'NNP', 'NNPS']
  lemmatizedtokens = []
  for token in tokenlist:
    if token[1] not in forbiddentags:
      lemmatizedtokens.append((lem.lemmatize(token[0]), token[1]))
    else:
      lemmatizedtokens.append(token)
  return lemmatizedtokens

# Racinisation en NLTK
def stem_tokens(tokenlist):
  forbiddentags = ['FW', 'NNP', 'NNPS']
  stemmedtokens = []
  for token in tokenlist:
    if token[1] not in forbiddentags:
      stemmedtokens.append((ps.stem(token[0]), token[1]))
    else:
      stemmedtokens.append(token)
  return stemmedtokens

# On ne garde que les mots (depuis NLTK)
def untag_tokens(tokenlist):
  untaggedtokens = [token[0] for token in tokenlist]
  return untaggedtokens

# Tokénisation avec NLTK réadaptée pour les pipelines
def tokenize_words_2(sentences):
  totaltokens = []
  for sentence in sentences:
    senttokens = word_tokenize(sentence)
    senttokens_tag = nltk.pos_tag(senttokens)
    for token in senttokens_tag:
      totaltokens.append(token)
  return totaltokens

# Fonctions à substituer en cas d'absence de tokénisation avec NLTK

# Tokenisateur classique: ne conserve que les mots
def classic_word_tokenizer(sentences):
  tokens = []
  for sentence in sentences:
    splitsentence = re.split('\s+', sentence)
    for word in splitsentence:
      tokens.append(word)
  return tokens

# Nettoyage de la ponctuation et d'autres symboles
def classic_clean_punctuation_symbols(tokenlist):
  newtokenlist = tokenlist
  for token in newtokenlist:
    re.sub("[(\[{)\]},\-\-.!?:;*$£€¥%&'+/]|\.\.\.|\"", '', token)
  return newtokenlist

# Suppression des stopwords
def classic_remove_stopwords(tokenlist):
  newtokenlist = [word for word in tokenlist if word not in the_stopwords]
  return newtokenlist

# Lemmatisation des mots
def classic_lemmatize_tokens(tokenlist):
  newtokenlist = [lem.lemmatize(word) for word in tokenlist]
  return newtokenlistNLTK

# Racinisation des mots
def classic_stem_tokens(tokenlist):
  newtokenlist = [ps.stem(word) for word in tokenlist]
  return newtokenlist

# Réassemblement des mots en un texte complet
def reassemble_tokens(tokenlist):
  fulltext = ""
  for word in tokenlist:
    fulltext += word+' '
  return fulltext

# Fonction de pré-traitement supplémentaire à inclure dans les pipelines

# Retirer les nombres écrits en chiffre
def remove_digits(tokenlist):
  newtokenlist = tokenlist
  for token in newtokenlist:
    re.sub("\d+", '', token)
  newtokenlist2 = [word for word in newtokenlist if word != ""]
  return newtokenlist2

"""**Définition des pipelines, des classifieurs et des fonctions associées**

Nous avons choisi de définir des pipelines pour faciliter l'application de différentes étapes de pré-traitement à nos données textuelles. Pour ce faire, nous avons pris l'exemple du notebook *Classification des données textuelles*: nous définissons une fonction qui applique les différentes étapes de pré-traitement sur le texte passé en paramètre, puis on crée une classe qui facilite l'instanciation des étapes sélectionnées. S'ensuivent la définition des différentes pipelines utilisées lors de la recherche de la meilleure pipeline pour chaque classification, puis la définition des différents classifieurs à évaluer à l'aide de la meilleure pipeline.
"""

# Creation de la fonction ProjectCleanText: active ou désactive les différentes étapes du pré-traitement

def ProjectCleanText(X,
                     removehyperlinks=False,
                     removeemails=False,
                     grammartokenizer=False,
                     removestopwords=False,
                     selectedtokens=alltags,
                     lemmatizetokens=False,
                     stemtokens=False,
                     removedigits=False
                    ):
  articletext = str(X)
  # Encoder en ASCII
  articletext = unicodedata.normalize('NFKD', articletext).encode('ascii', 'ignore')
  # Retirer les balises HTML
  articletext = strip_html(articletext)
  # Retirer les hyperliens
  if removehyperlinks:
    articletext = remove_hyperlink(articletext)
  # Retirer les adresses email
  if removeemails:
    articletext = remove_email(articletext)
  # Tokéniser en phrases
  articlesentences = tokenize_sentences(articletext)
  articletokens = []
  if grammartokenizer: # Si tokénisation grammaticale en tokens avec NLTK
    articletokens = tokenize_words_2(articlesentences)
    # Nettoyer la ponctuation en excès
    articletokens = clean_punctuation_symbols(articletokens)
    # Retirer les stopwords
    if removestopwords:
      articletokens = remove_stopwords(articletokens)
    # Sélectionner certains tokens selon leur classe grammaticale
    articletokens = select_tokens(articletokens, tags=selectedtokens)
    # Lemmatiser les tokens
    if lemmatizetokens:
      articletokens = lemmatize_tokens(articletokens)
    # Raciniser les tokens
    if stemtokens:
      articletokens = stem_tokens(articletokens)
    # Déclasser les tokens: on ne retient que les mots
    articletokens = untag_tokens(articletokens)
  else: # Sinon tokénisation classique en mots
    # Nettoyage de la ponctuation
    articletokens = classic_clean_punctuation_symbols(classic_word_tokenizer(articlesentences))
    # Retirer les stopwords
    if removestopwords:
      articletokens = classic_remove_stopwords(articletokens)
    # Lemmatiser les mots
    if lemmatizetokens:
      articletokens = classic_lemmatize_tokens(articletokens)
    # Raciniser les mots
    if stemtokens:
      articletokens = classic_stem_tokens(articletokens)
  # Retirer les chiffres dans les mots
  if removedigits:
    articletokens = remove_digits(articletokens)
  # Réassembler les mots en un texte entier pour la vectorisation
  articletext = reassemble_tokens(articletokens)
  return articletext

# Création de la classe TextNormalizer pour faciliter la création des pipelines

class TextNormalizer(BaseEstimator, TransformerMixin):
  def __init__(self,
               removehyperlinks=False,
               removeemails=False,
               grammartokenizer=False,
               removestopwords=False,
               selectedtokens=alltags,
               lemmatizetokens=False,
               stemtokens=False,
               removedigits=False
              ):

    self.removehyperlinks=removehyperlinks
    self.removeemails=removeemails
    self.grammartokenizer=grammartokenizer
    self.removestopwords=removestopwords
    self.selectedtokens=selectedtokens
    self.lemmatizetokens=lemmatizetokens
    self.stemtokens=stemtokens
    self.removedigits=removedigits

  def transform(self, X, **transform_params):
    X=X.copy()
    return [ProjectCleanText(text,
                             removehyperlinks=self.removehyperlinks,
                             removeemails=self.removeemails,
                             grammartokenizer=self.grammartokenizer,
                             removestopwords=self.removestopwords,
                             selectedtokens=self.selectedtokens,
                             lemmatizetokens=self.lemmatizetokens,
                             stemtokens=self.stemtokens,
                             removedigits=self.removedigits) for text in X]
  def fit(self, X, y=None, **fit_params):
    return self

  def fit_transform(self, X, y=None, **fit_params):
    return self.fit(X).transform(X)

  def get_params(self, deep=True):
    return {
      'removehyperlinks':self.removehyperlinks,
      'removeemails':self.removeemails,
      'grammartokenizer':self.grammartokenizer,
      'removestopwords':self.removestopwords,
      'selectedtokens':self.selectedtokens,
      'lemmatizetokens':self.lemmatizetokens,
      'stemtokens':self.stemtokens,
      'removedigits':self.removedigits
           }

  def set_params (self, **parameters):
    for parameter, value in parameters.items():
      setattr(self,parameter,value)
    return self

# Création des pipelines à tester avec SVC
# On cherche pour l'instant à trouver la meilleure pipeline de traitement pour chacune des classifications

# DECOMMENTER LA LIGNE CORRESPONDANTE
tags = alltags # tous les mots possibles à tokeniser via NLTK
#tags=['CD', 'FW', 'JJ', 'JJR', 'JJS', 'NN', 'NNP', 'NNS', 'NNPS', 'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'] # nombres, mots hors anglais, adjectifs, noms communs/propres, adverbes, verbes

# Pipelines utilisant CountVectorizer
CV = Pipeline([('cleaner', TextNormalizer()),
                    ('countvectorizer', CountVectorizer()),
                    ('svm', SVC())])
CV_noHL = Pipeline([('cleaner', TextNormalizer(removehyperlinks=True)),
                    ('countvectorizer', CountVectorizer()),
                    ('svm', SVC())])
CV_noHLem = Pipeline([('cleaner', TextNormalizer(removehyperlinks=True, removeemails=True)),
                    ('countvectorizer', CountVectorizer()),
                    ('svm', SVC())])
CV_noHLem_GT = Pipeline([('cleaner', TextNormalizer(removehyperlinks=True, removeemails=True, grammartokenizer=True)),
                    ('countvectorizer', CountVectorizer()),
                    ('svm', SVC())])
CV_noHLemSW_GT = Pipeline([('cleaner', TextNormalizer(removehyperlinks=True, removeemails=True, grammartokenizer=True, removestopwords=True)),
                    ('countvectorizer', CountVectorizer()),
                    ('svm', SVC())])
CV_noHLemSWST_GT = Pipeline([('cleaner', TextNormalizer(removehyperlinks=True, removeemails=True, grammartokenizer=True, removestopwords=True,
                                                       selectedtokens=tags)),
                    ('countvectorizer', CountVectorizer()),
                    ('svm', SVC())])
CV_noHLemSWST_GTlem = Pipeline([('cleaner', TextNormalizer(removehyperlinks=True, removeemails=True, grammartokenizer=True, removestopwords=True,
                                                       selectedtokens=tags, lemmatizetokens=True)),
                    ('countvectorizer', CountVectorizer()),
                    ('svm', SVC())])
CV_noHLemSWST_GTlemstem = Pipeline([('cleaner', TextNormalizer(removehyperlinks=True, removeemails=True, grammartokenizer=True, removestopwords=True,
                                                       selectedtokens=tags, lemmatizetokens=True, stemtokens=True)),
                    ('countvectorizer', CountVectorizer()),
                    ('svm', SVC())])
CV_noHLemSWSTdgt_GTlemstem = Pipeline([('cleaner', TextNormalizer(removehyperlinks=True, removeemails=True, grammartokenizer=True, removestopwords=True,
                                                       selectedtokens=tags, lemmatizetokens=True, stemtokens=True, removedigits=True)),
                    ('countvectorizer', CountVectorizer()),
                    ('svm', SVC())])

# Pipelines utilisant TfidfVectorizer
TF = Pipeline([('cleaner', TextNormalizer()),
                    ('countvectorizer', TfidfVectorizer()),
                    ('svm', SVC())])
TF_noHL = Pipeline([('cleaner', TextNormalizer(removehyperlinks=True)),
                    ('countvectorizer', TfidfVectorizer()),
                    ('svm', SVC())])
TF_noHLem = Pipeline([('cleaner', TextNormalizer(removehyperlinks=True, removeemails=True)),
                    ('countvectorizer', TfidfVectorizer()),
                    ('svm', SVC())])
TF_noHLem_GT = Pipeline([('cleaner', TextNormalizer(removehyperlinks=True, removeemails=True, grammartokenizer=True)),
                    ('countvectorizer', TfidfVectorizer()),
                    ('svm', SVC())])
TF_noHLemSW_GT = Pipeline([('cleaner', TextNormalizer(removehyperlinks=True, removeemails=True, grammartokenizer=True, removestopwords=True)),
                    ('countvectorizer', TfidfVectorizer()),
                    ('svm', SVC())])
TF_noHLemSWST_GT = Pipeline([('cleaner', TextNormalizer(removehyperlinks=True, removeemails=True, grammartokenizer=True, removestopwords=True,
                                                       selectedtokens=tags)),
                    ('countvectorizer', TfidfVectorizer()),
                    ('svm', SVC())])
TF_noHLemSWST_GTlem = Pipeline([('cleaner', TextNormalizer(removehyperlinks=True, removeemails=True, grammartokenizer=True, removestopwords=True,
                                                       selectedtokens=tags, lemmatizetokens=True)),
                    ('countvectorizer', TfidfVectorizer()),
                    ('svm', SVC())])
TF_noHLemSWST_GTlemstem = Pipeline([('cleaner', TextNormalizer(removehyperlinks=True, removeemails=True, grammartokenizer=True, removestopwords=True,
                                                       selectedtokens=tags, lemmatizetokens=True, stemtokens=True)),
                    ('countvectorizer', TfidfVectorizer()),
                    ('svm', SVC())])
TF_noHLemSWSTdgt_GTlemstem = Pipeline([('cleaner', TextNormalizer(removehyperlinks=True, removeemails=True, grammartokenizer=True, removestopwords=True,
                                                       selectedtokens=tags, lemmatizetokens=True, stemtokens=True, removedigits=True)),
                    ('countvectorizer', TfidfVectorizer()),
                    ('svm', SVC())])

# Array de pipelines
all_pipes = [
    ("CV", CV),
    ("CV_noHL", CV_noHL),
    ("CV_noHLem", CV_noHLem),
    ("CV_noHLem_GT", CV_noHLem_GT),
    ("CV_noHLemSW_GT", CV_noHLemSW_GT),
    ("CV_noHLemSWST_GT", CV_noHLemSWST_GT),
    ("CV_noHLemSWST_GTlem", CV_noHLemSWST_GTlem),
    ("CV_noHLemSWST_GTlemstem", CV_noHLemSWST_GTlemstem),
    ("CV_noHLemSWSTdgt_GTlemstem", CV_noHLemSWSTdgt_GTlemstem),
    ("TF", TF),
    ("TF_noHL", TF_noHL),
    ("TF_noHLem", TF_noHLem),
    ("TF_noHLem_GT", TF_noHLem_GT),
    ("TF_noHLemSW_GT", TF_noHLemSW_GT),
    ("TF_noHLemSWST_GT", TF_noHLemSWST_GT),
    ("TF_noHLemSWST_GTlem", TF_noHLemSWST_GTlem),
    ("TF_noHLemSWST_GTlemstem", TF_noHLemSWST_GTlemstem),
    ("TF_noHLemSWSTdgt_GTlemstem", TF_noHLemSWSTdgt_GTlemstem)
]

# Array de classifieurs
all_models = [
    ('MultinomialNB',MultinomialNB()),
    ('LR', LogisticRegression(solver='lbfgs')),
    ('KNN', KNeighborsClassifier()),
    ('CART', DecisionTreeClassifier()),
    ('RF', RandomForestClassifier()),
    ('SVM', SVC())
]

"""### 1. VRAI VS FAUX

Pour chacune des classifications, nous instancions les données utilisées et équilibrons celles-ci si nécessaire. Ensuite, nous évaluons la meilleure pipeline pour la classification correspondante, puis on utilise cette pipeline pour afficher la matrice de confusion sur les données. Nous utilisons alors l'objet TextNormalizer de la pipeline afin de traiter notre texte pour l'évaluation du meilleur classifieur, puis pour l'affichage des boites à moustache sur leur *accuracy*. Enfin, nous utilisons la meilleure pipeline et le meilleur classifieur pour rechercher les meilleurs hyperparamètres disponibles, afin d'obtenir la meilleure *accuracy* possible.

**Instanciation des données**
"""

dfclass1 = df.copy(deep=True)
dfclass1 = dfclass1.loc[(dfclass1['our rating']=='true') | (dfclass1['our rating']=='false')]
dfclass1.rename(columns={'our rating':'rating'}, inplace=True)

display(dfclass1)

# DECOMMENTER LES LIGNES CI-DESSOUS POUR EQUILIBRER LES DONNEES UTILISEES:
#rows_to_remove = dfclass1[dfclass1['rating'] == "false"].sample(n=893-421).index # 421 false, 421 true
#dfclass1 = dfclass1.drop(rows_to_remove)

print(dfclass1['rating'].value_counts())

Xclass1 = dfclass1.text
yclass1 = dfclass1.rating

# Création du jeu de données
trainsize = 0.7
testsize = 0.3
seed=30
Xtrain1, Xtest1, ytrain1, ytest1 = train_test_split(Xclass1, yclass1, train_size=trainsize, random_state=seed, test_size=testsize)

"""**Evaluation de la meilleure pipeline**"""

# Evaluation des différentes pipelines
# ATTENTION: TEST TRES LONG. COMPTER ENTRE 1 ET 2H.

print ("Evaluation des différentes configurations : ")
unsorted_scores = [(name, cross_val_score(model, Xclass1, yclass1, cv=5).mean()) for name, model in all_pipes]
scores = sorted(unsorted_scores, key=lambda x: -x[1])
print(scores)

# Remplacer la valeur de pipeline par le nom de la meilleure pipeline évaluée précédemment
pipeline=CV_noHLemSWST_GTlemstem # Laissé en exemple

# Affichage de la matrice de confusion correspondante
pipeline.fit(Xtrain1, ytrain1)
ypred1 = pipeline.predict(Xtest1)
MyshowAllScores(ytest1,ypred1)

"""**Evaluation du meilleur classifieur**"""

# Evaluation des différents classifieurs

score = 'accuracy'
seed = 7 # seed par défaut
allresults = []
results = []
names = []

# On normalise selon la meilleure pipeline évaluée précédemment
text_normalizer=TextNormalizer(removehyperlinks=True, removeemails=True, grammartokenizer=True, removestopwords=True,
                                                       selectedtokens=tags, lemmatizetokens=True, stemtokens=True)
Xclass1_cleaned=text_normalizer.fit_transform(Xclass1)
# DECOMMENTER POUR CHOISIR LE VECTORIZER CORRESPONDANT A LA MEILLEURE PIPELINE
tfidf=CountVectorizer()
#tfidf=TfidfVectorizer()
features=tfidf.fit_transform(Xclass1_cleaned).toarray()

for name,model in all_models:
    # cross validation en 10 fois
    kfold = KFold(n_splits=10, random_state=seed, shuffle=True)

    print ("Evaluation de ",name)
    start_time = time.time()
    # application de la classification
    cv_results = cross_val_score(model, features, yclass1, cv=kfold, scoring=score)

    thetime=time.time() - start_time
    result=Result(name,cv_results.mean(),cv_results.std(),thetime)
    allresults.append(result)

    # paramétrage de l'affichage
    results.append(cv_results)
    names.append(name)
    print("%s : %0.3f (%0.3f) in %0.3f s" % (name, cv_results.mean(), cv_results.std(),thetime))

allresults=sorted(allresults, key=lambda result: result.scoremean, reverse=True)

# affichage des résultats
print ('\nLe meilleur resultat : ')
print ('Classifier : ',allresults[0].name,
       ' %s : %0.3f' %(score,allresults[0].scoremean),
       ' (%0.3f)'%allresults[0].stdresult,
       ' en %0.3f '%allresults[0].timespent,' s\n')
print ('Tous les résultats : \n')
for result in allresults:
    print ('Classifier : ',result.name,
       ' %s : %0.3f' %(score,result.scoremean),
       ' (%0.3f)'%result.stdresult,
       ' en %0.3f '%result.timespent,' s')

# Graphe de comparaison des algorithmes (boites à moustache)

fig = plt.figure()
fig.suptitle('Comparaison des algorithmes')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names)

"""**Evaluation des meilleurs hyperparamètres**"""

# Préparation des données pour recherche des hyperparamètres
X_train1hyp, X_test1hyp, y_train1hyp, y_test1hyp = train_test_split(Xclass1, yclass1, test_size=0.1)

# Recherche des hyperparamètres
# ATTENTION: TRES LONG. COMPTER 1H MINIMUM.
pipeline=Pipeline([("cleaner", TextNormalizer(removehyperlinks=True, removeemails=True, grammartokenizer=True, removestopwords=True,
                                                       selectedtokens=tags, lemmatizetokens=True, stemtokens=True)), # objet TextNormalizer de la meilleure pipeline
                   ("CV", CountVectorizer()),
                   #("TFIDF", TfidfVectorizer()),
                   ('LR', LogisticRegression())
                   #('RF', RandomForestClassifier())
                   #('SVM', SVC())
                  ])
parameters = {
    # Pour SVM
    #'SVM__C': [0.001, 0.01, 0.1, 1, 10],
    #'SVM__gamma' : [0.001, 0.01, 0.1, 1],
    #'SVM__kernel': ['linear','rbf','poly','sigmoid']

    # Pour LR
    'LR__solver' : ['newton-cg', 'lbfgs', 'liblinear'],
    'LR__penalty' : ['l2'],
    'LR__C' : [100, 10, 1.0, 0.1, 0.01],

    # Pour RF
    #'RF__n_estimators': [500, 1200],
    #'RF__max_depth': [25, 30],
    #'RF__min_samples_split': [5, 10, 15],
    #'RF__min_samples_leaf' : [1, 2],
}

score='accuracy'
grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1,  verbose=1,scoring=score)

print("Application de gridsearch ...")
print("pipeline :", [name for name, _ in pipeline.steps])
print("parameters :")
print(parameters)
start_time = time.time()
grid_search.fit(X_train1hyp, y_train1hyp)
print("réalisé en  %0.3f s" % (time.time() - start_time))
print("Meilleur résultat : %0.3f" % grid_search.best_score_)

# autres mesures et matrice de confusion
y_pred1hyp = grid_search.predict(X_test1hyp)
MyshowAllScores(y_test1hyp,y_pred1hyp)

print("Ensemble des meilleurs paramètres :")
best_parameters = grid_search.best_estimator_.get_params()
for param_name in sorted(parameters.keys()):
        print("\t%s: %r" % (param_name, best_parameters[param_name]))

# Affichage des premiers résultats du gridsearch
df_results=pd.concat([pd.DataFrame(grid_search.cv_results_["params"]),
           pd.DataFrame(grid_search.cv_results_["mean_test_score"],
                        columns=[score])],axis=1).sort_values(score,ascending=False)
print ("\nLes premiers résultats : \n",df_results.head())

"""### 2. VRAI OU FAUX VS AUTRE

Pour chacune des classifications, nous instancions les données utilisées et équilibrons celles-ci si nécessaire. Ensuite, nous évaluons la meilleure pipeline pour la classification correspondante, puis on utilise cette pipeline pour afficher la matrice de confusion sur les données. Nous utilisons alors l'objet TextNormalizer de la pipeline afin de traiter notre texte pour l'évaluation du meilleur classifieur, puis pour l'affichage des boites à moustache sur leur *accuracy*. Enfin, nous utilisons la meilleure pipeline et le meilleur classifieur pour rechercher les meilleurs hyperparamètres disponibles, afin d'obtenir la meilleure *accuracy* possible.
"""

# Définition d'une fonction qui rassemble vrai et faux
# On l'appliquera sur toute la colonne rating
def truefalseunion(rating):
  newrating = rating
  if rating == 'true' or rating == 'false':
    newrating = 'torf' # true or false
  return newrating

"""**Instanciation des données**"""

dfclass2 = df.copy(deep=True)
dfclass2 = dfclass2.loc[(dfclass2['our rating']=='true') | (dfclass2['our rating']=='false') | (dfclass2['our rating']=='other')]
dfclass2['our rating'] = dfclass2['our rating'].apply(lambda x: truefalseunion(x))
dfclass2.rename(columns={'our rating':'rating'}, inplace=True)

display(dfclass2)

# A DECOMMENTER POUR EQUILIBRER LES DONNEES UTILISEES:
#rows_to_remove = dfclass2[dfclass2['rating'] == 'torf'].sample(n=1314-148).index # 148 other, 148 torf
# Décommenter ligne ci-dessous pour partage des jeux: le but est de rééquilibrer la matrice
#rows_to_remove = dfclass2[dfclass2['rating'] == 'torf'].sample(n=1314-(148 - int(148/16))).index

dfclass2 = dfclass2.drop(rows_to_remove)

print(dfclass2['rating'].value_counts())

Xclass2 = dfclass2.text
yclass2 = dfclass2.rating

# Création du jeu de données
trainsize2 = 0.7
testsize2 = 0.3
seed=30
Xtrain2, Xtest2, ytrain2, ytest2 = train_test_split(Xclass2, yclass2, train_size=trainsize2, random_state=seed, test_size=testsize2)

"""**Evaluation de la meilleure pipeline**"""

# Evaluation des différentes pipelines
# ATTENTION: TEST TRES LONG. COMPTER 1 A 2H.

print ("Evaluation des différentes configurations : ")
unsorted_scores = [(name, cross_val_score(model, Xclass2, yclass2, cv=5).mean()) for name, model in all_pipes]
scores = sorted(unsorted_scores, key=lambda x: -x[1])
print(scores)

# Remplacer la valeur de pipeline par la meilleure pipeline évaluée précédemment
pipeline=TF_noHLemSW_GT # Laissé en exemple

# Affichage de la matrice de confusion sur la meilleure pipeline évaluée
pipeline.fit(Xtrain2, ytrain2)
ypred2 = pipeline.predict(Xtest2)
MyshowAllScores(ytest2,ypred2)

"""**Evaluation du meilleur classifieur**"""

# Evaluation des différents classifieurs

score = 'accuracy'
seed = 7
allresults = []
results = []
names = []

# On normalise selon la meilleure pipeline évaluée précédemment
text_normalizer=TextNormalizer(removehyperlinks=True, removeemails=True, grammartokenizer=True, removestopwords=True,
                                                       selectedtokens=tags, lemmatizetokens=True)
Xclass2_cleaned=text_normalizer.fit_transform(Xclass2)
# DECOMMENTER POUR CHOISIR LE VECTORIZER CORRESPONDANT A LA MEILLEURE PIPELINE
#tfidf=CountVectorizer()
tfidf=TfidfVectorizer()
features=tfidf.fit_transform(Xclass2_cleaned).toarray()

for name,model in all_models:
    # cross validation en 10 fois
    kfold = KFold(n_splits=10, random_state=seed, shuffle=True)

    print ("Evaluation de ",name)
    start_time = time.time()
    # application de la classification
    cv_results = cross_val_score(model, features, yclass2, cv=kfold, scoring=score)

    thetime=time.time() - start_time
    result=Result(name,cv_results.mean(),cv_results.std(),thetime)
    allresults.append(result)

    # paramétrage de l'affichage
    results.append(cv_results)
    names.append(name)
    print("%s : %0.3f (%0.3f) in %0.3f s" % (name, cv_results.mean(), cv_results.std(),thetime))

allresults=sorted(allresults, key=lambda result: result.scoremean, reverse=True)

# affichage des résultats
print ('\nLe meilleur resultat : ')
print ('Classifier : ',allresults[0].name,
       ' %s : %0.3f' %(score,allresults[0].scoremean),
       ' (%0.3f)'%allresults[0].stdresult,
       ' en %0.3f '%allresults[0].timespent,' s\n')
print ('Tous les résultats : \n')
for result in allresults:
    print ('Classifier : ',result.name,
       ' %s : %0.3f' %(score,result.scoremean),
       ' (%0.3f)'%result.stdresult,
       ' en %0.3f '%result.timespent,' s')

# Graphe de comparaison des algorithmes (boites à moustache)

fig = plt.figure()
fig.suptitle('Comparaison des algorithmes')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names)

"""**Evaluation des meilleurs hyperparamètres**"""

# Préparation des données pour recherche des hyperparamètres
X_train2hyp, X_test2hyp, y_train2hyp, y_test2hyp = train_test_split(Xclass2, yclass2, test_size=0.1)

# Recherche des hyperparamètres
# ATTENTION: TRES LONG. COMPTER 1H MINIMUM.
pipeline=Pipeline([("cleaner", TextNormalizer(removehyperlinks=True, removeemails=True, grammartokenizer=True, removestopwords=True,
                                                       selectedtokens=tags, lemmatizetokens=True)), # objet TextNormalizer de la meilleure pipeline
                   #("CV", CountVectorizer()),
                   ("TFIDF", TfidfVectorizer()),
                   ('LR', LogisticRegression())
                   #('RF', RandomForestClassifier())
                   #('SVM', SVC())
                  ])
parameters = {
    # Pour SVM
    #'SVM__C': [0.001, 0.01, 0.1, 1, 10],
    #'SVM__gamma' : [0.001, 0.01, 0.1, 1],
    #'SVM__kernel': ['linear','rbf','poly','sigmoid']

    # Pour LR
    'LR__solver' : ['newton-cg', 'lbfgs', 'liblinear'],
    'LR__penalty' : ['l2'],
    'LR__C' : [100, 10, 1.0, 0.1, 0.01],

    # Pour RF
    #'RF__n_estimators': [500, 1200],
    #'RF__max_depth': [25, 30],
    #'RF__min_samples_split': [5, 10, 15],
    #'RF__min_samples_leaf' : [1, 2],
}

score='accuracy'
grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1,  verbose=1,scoring=score)

print("Application de gridsearch ...")
print("pipeline :", [name for name, _ in pipeline.steps])
print("parameters :")
print(parameters)
start_time = time.time()
grid_search.fit(X_train2hyp, y_train2hyp)
print("réalisé en  %0.3f s" % (time.time() - start_time))
print("Meilleur résultat : %0.3f" % grid_search.best_score_)

# autres mesures et matrice de confusion
y_pred2hyp = grid_search.predict(X_test2hyp)
MyshowAllScores(y_test2hyp,y_pred2hyp)

print("Ensemble des meilleurs paramètres :")
best_parameters = grid_search.best_estimator_.get_params()
for param_name in sorted(parameters.keys()):
        print("\t%s: %r" % (param_name, best_parameters[param_name]))

# Affichage des premiers résultats du gridsearch
df_results=pd.concat([pd.DataFrame(grid_search.cv_results_["params"]),
           pd.DataFrame(grid_search.cv_results_["mean_test_score"],
                        columns=[score])],axis=1).sort_values(score,ascending=False)
print ("\nLes premiers résultats : \n",df_results.head())

"""### 3. VRAI VS FAUX VS MIXTE VS AUTRE

Pour chacune des classifications, nous instancions les données utilisées et équilibrons celles-ci si nécessaire. Ensuite, nous évaluons la meilleure pipeline pour la classification correspondante, puis on utilise cette pipeline pour afficher la matrice de confusion sur les données. Nous utilisons alors l'objet TextNormalizer de la pipeline afin de traiter notre texte pour l'évaluation du meilleur classifieur, puis pour l'affichage des boites à moustache sur leur *accuracy*. Enfin, nous utilisons la meilleure pipeline et le meilleur classifieur pour rechercher les meilleurs hyperparamètres disponibles, afin d'obtenir la meilleure *accuracy* possible.

**Instanciation des données**
"""

dfclass3 = df.copy(deep=True)
dfclass3.rename(columns={'our rating':'rating'}, inplace=True)
display(dfclass3)

# DECOMMENTER POUR EQUILIBRER LES DONNEES UTILISEES:
#rows_to_remove = dfclass3[dfclass3['rating'] == "false"].sample(n=893-148).index # 148 false
#dfclass3 = dfclass3.drop(rows_to_remove)
#rows_to_remove = dfclass3[dfclass3['rating'] == "true"].sample(n=421-148).index # 148 true
#dfclass3 = dfclass3.drop(rows_to_remove)
#rows_to_remove = dfclass3[dfclass3['rating'] == "mixture"].sample(n=414-148).index # 148 mixture
#dfclass3 = dfclass3.drop(rows_to_remove)

#rows_to_remove = dfclass3[dfclass3['rating'] == "false"].sample(n=893-148+int(148/32)).index # 144 false
#dfclass3 = dfclass3.drop(rows_to_remove)
#rows_to_remove = dfclass3[dfclass3['rating'] == "true"].sample(n=421-148+int(148/24)).index # 142 true
#dfclass3 = dfclass3.drop(rows_to_remove)
#rows_to_remove = dfclass3[dfclass3['rating'] == "mixture"].sample(n=414-148+int(148/64)).index # 146 mixture
#dfclass3 = dfclass3.drop(rows_to_remove)

print(dfclass3['rating'].value_counts())

Xclass3 = dfclass3.text
yclass3 = dfclass3.rating

# Création du jeu de données
trainsize = 0.7
testsize = 0.3
seed=30
Xtrain3, Xtest3, ytrain3, ytest3 = train_test_split(Xclass3, yclass3, train_size=trainsize, random_state=seed, test_size=testsize)

"""**Evaluation de la meilleure pipeline**"""

# Evaluation des différentes pipelines
# ATTENTION: TEST TRES LONG. COMPTER 2 A 3H.

print ("Evaluation des différentes configurations : ")
unsorted_scores = [(name, cross_val_score(model, Xclass3, yclass3, cv=5).mean()) for name, model in all_pipes]
scores = sorted(unsorted_scores, key=lambda x: -x[1])
print(scores)

# Remplacer la valeur de pipeline par la meilleure pipeline évaluée précédemment
pipeline=TF_noHLemSWST_GTlem # Laissé en exemple

# Affichage de la matrice de confusion sur le modèle
pipeline.fit(Xtrain3, ytrain3)
ypred3 = pipeline.predict(Xtest3)
MyshowAllScores(ytest3,ypred3)

"""**Evaluation du meilleur classifieur**"""

# Evaluation des différents classifieurs

score = 'accuracy'
seed = 7 # seed par défaut
allresults = []
results = []
names = []

# On normalise selon la meilleure pipeline évaluée précédemment
text_normalizer=TextNormalizer(removehyperlinks=True, removeemails=True, grammartokenizer=True, removestopwords=True,
                                                       selectedtokens=tags, lemmatizetokens=True)
Xclass3_cleaned=text_normalizer.fit_transform(Xclass3)
# DECOMMENTER POUR CHOISIR LE VECTORIZER CORRESPONDANT A LA MEILLEURE PIPELINE
#tfidf=CountVectorizer()
tfidf=TfidfVectorizer()
features=tfidf.fit_transform(Xclass3_cleaned).toarray()

for name,model in all_models:
    # cross validation en 10 fois
    kfold = KFold(n_splits=10, random_state=seed, shuffle=True)

    print ("Evaluation de ",name)
    start_time = time.time()
    # application de la classification
    cv_results = cross_val_score(model, features, yclass3, cv=kfold, scoring=score)

    thetime=time.time() - start_time
    result=Result(name,cv_results.mean(),cv_results.std(),thetime)
    allresults.append(result)

    # paramétrage de l'affichage
    results.append(cv_results)
    names.append(name)
    print("%s : %0.3f (%0.3f) in %0.3f s" % (name, cv_results.mean(), cv_results.std(),thetime))

allresults=sorted(allresults, key=lambda result: result.scoremean, reverse=True)

# affichage des résultats
print ('\nLe meilleur resultat : ')
print ('Classifier : ',allresults[0].name,
       ' %s : %0.3f' %(score,allresults[0].scoremean),
       ' (%0.3f)'%allresults[0].stdresult,
       ' en %0.3f '%allresults[0].timespent,' s\n')
print ('Tous les résultats : \n')
for result in allresults:
    print ('Classifier : ',result.name,
       ' %s : %0.3f' %(score,result.scoremean),
       ' (%0.3f)'%result.stdresult,
       ' en %0.3f '%result.timespent,' s')

# Graphe de comparaison des algorithmes (boites à moustache)

fig = plt.figure()
fig.suptitle('Comparaison des algorithmes')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names)

"""**Evaluation des meilleurs hyperparamètres**"""

# Préparation des données pour recherche des hyperparamètres
X_train3hyp, X_test3hyp, y_train3hyp, y_test3hyp = train_test_split(Xclass3, yclass3, test_size=0.1)

# Recherche des hyperparamètres
# ATTENTION: TRES LONG. COMPTER 1H MINIMUM.
pipeline=Pipeline([("cleaner", TextNormalizer(removehyperlinks=True, removeemails=True, grammartokenizer=True, removestopwords=True,
                                                       selectedtokens=tags, lemmatizetokens=True)), # objet TextNormalizer de la meilleure pipeline
                   #("CV", CountVectorizer()),
                   ("TFIDF", TfidfVectorizer()),
                   ('LR', LogisticRegression())
                   #('RF', RandomForestClassifier())
                   #('SVM', SVC())
                  ])
parameters = {
    # Pour SVM
    #'SVM__C': [0.001, 0.01, 0.1, 1, 10],
    #'SVM__gamma' : [0.001, 0.01, 0.1, 1],
    #'SVM__kernel': ['linear','rbf','poly','sigmoid']

    # Pour LR
    'LR__solver' : ['newton-cg'],
    'LR__penalty' : ['l2'],
    'LR__C' : [1.0],
    #'LR__solver' : ['newton-cg', 'lbfgs', 'liblinear'],
    #'LR__penalty' : ['l2'],
    #'LR__C' : [100, 10, 1.0, 0.1, 0.01],

    # Pour RF
    #'RF__n_estimators': [500, 1200],
    #'RF__max_depth': [25, 30],
    #'RF__min_samples_split': [5, 10, 15],
    #'RF__min_samples_leaf' : [1, 2],
}

score='accuracy'
grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1,  verbose=1,scoring=score)

print("Application de gridsearch ...")
print("pipeline :", [name for name, _ in pipeline.steps])
print("parameters :")
print(parameters)
start_time = time.time()
grid_search.fit(X_train3hyp, y_train3hyp)
print("réalisé en  %0.3f s" % (time.time() - start_time))
print("Meilleur résultat : %0.3f" % grid_search.best_score_)

# autres mesures et matrice de confusion
y_pred3hyp = grid_search.predict(X_test3hyp)
MyshowAllScores(y_test3hyp,y_pred3hyp)

print("Ensemble des meilleurs paramètres :")
best_parameters = grid_search.best_estimator_.get_params()
for param_name in sorted(parameters.keys()):
        print("\t%s: %r" % (param_name, best_parameters[param_name]))

# Affichage des premiers résultats du gridsearch
df_results=pd.concat([pd.DataFrame(grid_search.cv_results_["params"]),
           pd.DataFrame(grid_search.cv_results_["mean_test_score"],
                        columns=[score])],axis=1).sort_values(score,ascending=False)
print ("\nLes premiers résultats : \n",df_results.head())

"""## Etape 4 - Analyse des erreurs, validation, comparaison des modèles

**La grande majorité de l'analyse faite sur les données se trouve dans le rapport associé à ce projet. Dû aux limitations de Google Colaboratory et par soucis de concision, cette partie ne sera pas écrite de nouveau dans ce notebook.**

Par faute de place dans le rapport, nous n'avons pas pu mentionner la tentative de rectification des modèles en partant des matrices de confusion, notamment celle qui concerne la troisième classification.

Pour celles-ci, nous avons décidé d'établir un nouvel équilibre des données: au lieu d'équilibrer les données en les rendant totalement équitables, nous avons voulu biaiser les données dont la classification était auparavant plus ambigüe.

Ainsi, nous avons lancé les différentes évaluations sur les pipelines, les classifieurs et les hyperparamètres. Les résultats sont les suivants:

- classification 2 avec nouveau partage des jeux: **148 Autre VS 139 VouF** (*ba2*)

```
('TF_noHLemSWST_GTlem', 0.6024198427102239)
	accuracy matrice: 0.6437
	precision matrice: 0.6438
	recall matrice: 0.6438
	meilleur classifieur: LR accuracy 0.651
	meilleurs hyperparamètres:
		LR__C=10,
		LR__penalty=l2,
		LR__solver=newton-cg,
		accuracy=0.6510,
		matrix accuracy=0.6897,
		matrix precision=0.6587,
		matrix recall=0.6571
```

- classification 3 avec nouveau partage des jeux: **148 Autre VS 146 Mixte VS 144 Faux VS 142 Vrai** (*ba1*)

```
('TF_noHLemSWST_GTlem', 0.4275862068965517)
	accuracy matrice: 0.4885
	precision matrice: 0.4544
	recall matrice: 0.4447
	meilleur classifieur: LR accuracy 0.509
	meilleurs hyperparamètres:
		LR__C=1.0,
		LR__penalty=l2,
		LR__solver=newton-cg,
		accuracy=0.5210,
		matrix accuracy=0.4655,
		matrix precision=0.4574,
		matrix recall=0.4676
```

Les raisons derrière le choix de la pipeline, des classifieurs et des hyperparamètres sont similaires aux raisons derrière les choix effectués dans la section **Equilibrage des données** du rapport associé au projet. Ainsi, nous ne les détaillerons pas.

Toutes les images de matrice utilisées dans le rapport sont issues de ce notebook; néanmoins, par soucis de lisibilité, chacune des images utilisées sera disponible en taille réelle dans l'archive concernant le projet. De plus, les images annotées *ba1* et *ba2* qui concernent les nouveaux équilibres des jeux sur les classifications 3 et 2 respectivement y seront également disponibles.
"""